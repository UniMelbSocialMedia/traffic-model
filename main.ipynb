{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "\n",
    "from data_loader.data_loader import DataLoader\n",
    "from models.sgat_transformer.sgat_transformer import SGATTransformer\n",
    "from test import test\n",
    "from train import train\n",
    "from utils.data_utils import create_lookup_index\n",
    "from utils.logger import logger\n",
    "from utils.loss_func import Masked_MAE_Loss, Huber_Loss\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _train(model, configs, lr, ls_fn, is_lr_sh=True, _train=True):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if is_lr_sh:\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer=optimizer, T_0=20, T_mult=1,\n",
    "                                                                            eta_min=0.00001)\n",
    "        # lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=2, gamma=0.75)\n",
    "        lr_rates = [0.001, 0.0008875, 0.0007750000000000001, 0.0006625, 0.00055, 0.0004375, 0.000325, 0.0002125, 0.0001,\n",
    "                    9.17e-05, 8.340000000000001e-05, 7.510000000000001e-05, 6.68e-05, 5.8500000000000006e-05, 5.02e-05]\n",
    "\n",
    "        # lr_scheduler = CustomLRScheduler(optimizer, lr_rates, 15)\n",
    "\n",
    "    best_model_path = None\n",
    "    min_val_loss = np.inf\n",
    "    dec_offset = configs['transformer']['decoder']['seq_offset']\n",
    "    epochs = configs['train_epochs'] if _train else configs['finetune_epochs']\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # if is_lr_sh:\n",
    "            # logger.info(f\"LR: {lr_scheduler.get_last_lr()}\")\n",
    "\n",
    "        mae_train_loss, rmse_train_loss, mape_train_loss = train(model=model,\n",
    "                                                                 data_loader=data_loader,\n",
    "                                                                 optimizer=optimizer,\n",
    "                                                                 loss_fn=ls_fn,\n",
    "                                                                 device=configs['device'],\n",
    "                                                                 seq_offset=dec_offset,\n",
    "                                                                 _train=_train)\n",
    "\n",
    "        mae_val_loss, rmse_val_loss, mape_val_loss = test(_type='test',\n",
    "                                                          model=model,\n",
    "                                                          data_loader=data_loader,\n",
    "                                                          device=configs['device'],\n",
    "                                                          seq_offset=dec_offset)\n",
    "        if is_lr_sh:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        out_txt = f\"Epoch: {epoch} | mae_train_loss: {mae_train_loss} | rmse_train_loss: {rmse_train_loss} \" \\\n",
    "                  f\"| mape_train_loss: {mape_train_loss} | mae_val_loss: {mae_val_loss} \" \\\n",
    "                  f\"| rmse_val_loss: {rmse_val_loss} | mape_val_loss: {mape_val_loss}\"\n",
    "        logger.info(out_txt)\n",
    "\n",
    "        if min_val_loss > mae_val_loss:\n",
    "            min_val_loss = mae_val_loss\n",
    "            logger.info('Saving Model...')\n",
    "            best_model_path = configs['model_output_path'].format(str(epoch))\n",
    "            torch.save(model.state_dict(), best_model_path)  # saving model\n",
    "\n",
    "    return best_model_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_validate(model, configs: dict, data_loader: DataLoader):\n",
    "    if configs['load_saved_model']:\n",
    "        model.load_state_dict(torch.load(configs['model_input_path']))\n",
    "\n",
    "    # mse_loss_fn = nn.L1Loss()\n",
    "    mse_loss_fn = Masked_MAE_Loss()\n",
    "    huber_loss_fn = Huber_Loss(delta=1.0)\n",
    "\n",
    "    # Initial Training\n",
    "    # _train(model=model,\n",
    "    #        configs=configs,\n",
    "    #        lr=0.001,\n",
    "    #        ls_fn=mse_loss_fn,\n",
    "    #        is_lr_sh=True,\n",
    "    #        _train=True)\n",
    "\n",
    "    # Fine tuning\n",
    "    best_model_path = _train(model=model,\n",
    "                             configs=configs,\n",
    "                             lr=0.0005,\n",
    "                             ls_fn=mse_loss_fn,\n",
    "                             is_lr_sh=True,\n",
    "                             _train=False)\n",
    "\n",
    "    # testing model\n",
    "    logger.info('Testing model...')\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    dec_offset = configs['transformer']['decoder']['seq_offset']\n",
    "    mae_test_loss, rmse_test_loss, mape_test_loss = test(_type='test',\n",
    "                                                         model=model,\n",
    "                                                         data_loader=data_loader,\n",
    "                                                         device=configs['device'],\n",
    "                                                         seq_offset=dec_offset)\n",
    "\n",
    "    logger.info(f\"mae_test_loss: {mae_test_loss} | rmse_test_loss: {rmse_test_loss} | mape_test_loss: {mape_test_loss}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_data(model_configs: dict, data_configs: dict):\n",
    "    data_configs['batch_size'] = model_configs['batch_size']\n",
    "    data_configs['enc_features'] = model_configs['transformer']['encoder']['features']\n",
    "    data_configs['dec_seq_offset'] = model_configs['transformer']['decoder']['seq_offset']\n",
    "    dec_seq_len = model_configs['transformer']['decoder']['seq_len']\n",
    "    enc_seq_len = model_configs['transformer']['encoder']['seq_len']\n",
    "\n",
    "    data_loader = DataLoader(data_configs)\n",
    "    data_loader.load_node_data_file()\n",
    "    edge_index, edge_attr = data_loader.load_edge_data_file()\n",
    "    edge_details = data_loader.load_semantic_edge_data_file()\n",
    "\n",
    "    model_configs['transformer']['decoder']['edge_index'] = edge_index\n",
    "    model_configs['transformer']['decoder']['edge_attr'] = edge_attr\n",
    "    model_configs['transformer']['decoder']['edge_details'] = edge_details\n",
    "\n",
    "    model_configs['transformer']['encoder']['edge_index'] = edge_index\n",
    "    model_configs['transformer']['encoder']['edge_attr'] = edge_attr\n",
    "    model_configs['transformer']['encoder']['edge_details'] = edge_details\n",
    "    model_configs['transformer']['encoder']['num_of_weeks'] = data_configs['num_of_weeks']\n",
    "    model_configs['transformer']['encoder']['num_of_days'] = data_configs['num_of_days']\n",
    "    model_configs['transformer']['encoder']['basic_input_len'] = data_configs['len_input']\n",
    "    model_configs['transformer']['encoder']['points_per_hour'] = data_configs['points_per_hour']\n",
    "    model_configs['transformer']['encoder']['num_days_per_week'] = data_configs['num_days_per_week']\n",
    "\n",
    "    max_lkup_len_enc, lkup_idx_enc, max_lkup_len_dec, lkup_idx_dec = create_lookup_index(data_configs['num_of_weeks'],\n",
    "                                                                                         data_configs['num_of_days'],\n",
    "                                                                                         data_configs['dec_seq_offset'],\n",
    "                                                                                         dec_seq_len)\n",
    "\n",
    "    model_configs['transformer']['decoder']['lookup_idx'] = lkup_idx_dec\n",
    "    model_configs['transformer']['decoder']['max_lookup_len'] = max_lkup_len_dec if max_lkup_len_dec else dec_seq_len\n",
    "    model_configs['transformer']['encoder']['lookup_idx'] = lkup_idx_enc\n",
    "    model_configs['transformer']['encoder']['max_lookup_len'] = max_lkup_len_enc if max_lkup_len_enc else enc_seq_len\n",
    "\n",
    "    return data_loader, model_configs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # load configs\n",
    "    with open(\"config/config.yaml\", \"r\") as stream:\n",
    "        configs = yaml.safe_load(stream)\n",
    "\n",
    "    model_configs = configs['model']\n",
    "    data_configs = configs['data']\n",
    "    data_loader, model_configs = prepare_data(model_configs, data_configs)\n",
    "\n",
    "    model = SGATTransformer(configs=model_configs).to(model_configs['device'])\n",
    "    train_validate(model, model_configs, data_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
